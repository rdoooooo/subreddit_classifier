{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    epochs=2,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"science\", \"funny\"]\n",
    "\n",
    "# label_cols = ['science', 'funny', 'engineering', 'compsci',\n",
    "#                 'machinelearning', 'datascience', 'math', 'statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "\n",
    "class RedditDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token],\n",
    "                         labels: np.ndarray=None,\n",
    "                        id: str=None,) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        fields[\"label\"] = ArrayField(array=labels)\n",
    "    \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "        \n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # Reads in the pickle file and inputs the document, label, and id to make an instance\n",
    "        df = pd.read_pickle(file_path)\n",
    "        #df = df[df.labels<2]\n",
    "        # Imports data into the tokenizer\n",
    "        for index, row in df.iterrows():\n",
    "            #print(row.documents)\n",
    "            yield self.text_to_instance([Token(x) for x in self.tokenizer(row.documents)],\n",
    "                                        row[label_cols].values,\n",
    "                                        index)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=\"bert-base-uncased\",\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "# apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = RedditDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:01, 1165.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#train_ds, test_ds = (reader.read(fname) for fname in ['reddit_thread_label_small','reddit_thread_label_small'])\n",
    "train_ds = reader.read('reddit_thread_label_small_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [[UNK],\n",
       "  are,\n",
       "  using,\n",
       "  imaging,\n",
       "  tests,\n",
       "  to,\n",
       "  show,\n",
       "  for,\n",
       "  the,\n",
       "  first,\n",
       "  time,\n",
       "  that,\n",
       "  fr,\n",
       "  ##uc,\n",
       "  ##tose,\n",
       "  can,\n",
       "  trigger,\n",
       "  brain,\n",
       "  changes,\n",
       "  that,\n",
       "  may,\n",
       "  lead,\n",
       "  to,\n",
       "  over,\n",
       "  ##ea,\n",
       "  ##ting,\n",
       "  ##.,\n",
       "  [UNK],\n",
       "  is,\n",
       "  a,\n",
       "  sugar,\n",
       "  that,\n",
       "  sat,\n",
       "  ##ura,\n",
       "  ##tes,\n",
       "  the,\n",
       "  [UNK],\n",
       "  diet,\n",
       "  ##.],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer at 0x133381358>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocabulary\n",
    "\n",
    "We don't need to build the vocab: all that is handled by the token indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare iterator\n",
    "The iterator is responsible for batching the data and preparing it for input into the model. We'll use the BucketIterator that batches text sequences of smilar lengths together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell the iterator how to numericalize the text data. We do this by passing the vocabulary to the iterator. This step is easy to forget so be careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,   100,  2777,  ...,     0,     0,     0],\n",
       "          [  101,   100,  2302,  ...,   102,     0,     0],\n",
       "          [  101,   100,  2738,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [  101,   100,  2424,  ...,  9463, 18075,   102],\n",
       "          [  101,   100,  3062,  ..., 19728, 29625,   102],\n",
       "          [  101,   100,   100,  ...,     0,     0,     0]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ..., 15,  0,  0],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  2,  3,  ..., 15, 16, 17],\n",
       "          [ 1,  2,  3,  ..., 15, 16, 17],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0]]),\n",
       "  'tokens-type-ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]),\n",
       "  'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'label': tensor([[0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]]),\n",
       " 'id': [1464,\n",
       "  515,\n",
       "  171,\n",
       "  1208,\n",
       "  7,\n",
       "  648,\n",
       "  889,\n",
       "  707,\n",
       "  1717,\n",
       "  418,\n",
       "  1593,\n",
       "  1629,\n",
       "  276,\n",
       "  1577,\n",
       "  943,\n",
       "  1374,\n",
       "  936,\n",
       "  312,\n",
       "  1175,\n",
       "  82,\n",
       "  571,\n",
       "  1938,\n",
       "  1508,\n",
       "  898,\n",
       "  776,\n",
       "  1274,\n",
       "  126,\n",
       "  412,\n",
       "  906,\n",
       "  129,\n",
       "  1867,\n",
       "  6,\n",
       "  34,\n",
       "  532,\n",
       "  1307,\n",
       "  1587,\n",
       "  1640,\n",
       "  549,\n",
       "  508,\n",
       "  374,\n",
       "  1962,\n",
       "  969,\n",
       "  554,\n",
       "  329,\n",
       "  1365,\n",
       "  1410,\n",
       "  1025,\n",
       "  1241,\n",
       "  604,\n",
       "  1053,\n",
       "  940,\n",
       "  35,\n",
       "  673,\n",
       "  1137,\n",
       "  1655,\n",
       "  636,\n",
       "  1924,\n",
       "  1037,\n",
       "  1347,\n",
       "  291,\n",
       "  174,\n",
       "  597,\n",
       "  1420,\n",
       "  95]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(iterator(train_ds)))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   100,  2777,  ...,     0,     0,     0],\n",
       "        [  101,   100,  2302,  ...,   102,     0,     0],\n",
       "        [  101,   100,  2738,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   100,  2424,  ...,  9463, 18075,   102],\n",
       "        [  101,   100,  3062,  ..., 19728, 29625,   102],\n",
       "        [  101,   100,   100,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 19])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=len(label_cols)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [01:10<00:00, 5777609.31B/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "        top_layer_only=True, # conserve memory\n",
    ")\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                            # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                           allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "class BertSentencePooler(Seq2VecEncoder):\n",
    "    def forward(self, embs: torch.tensor, \n",
    "                mask: torch.tensor=None) -> torch.tensor:\n",
    "        # extract first token tensor\n",
    "        return embs[:, 0]\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return BERT_DIM\n",
    "    \n",
    "encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how simple and modular the code for initializing the model is. All the complexity is delegated to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[  101,   100,  2777,  ...,     0,     0,     0],\n",
       "         [  101,   100,  2302,  ...,   102,     0,     0],\n",
       "         [  101,   100,  2738,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   100,  2424,  ...,  9463, 18075,   102],\n",
       "         [  101,   100,  3062,  ..., 19728, 29625,   102],\n",
       "         [  101,   100,   100,  ...,     0,     0,     0]]),\n",
       " 'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ..., 15,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  2,  3,  ..., 15, 16, 17],\n",
       "         [ 1,  2,  3,  ..., 15, 16, 17],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0]]),\n",
       " 'tokens-type-ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1824, -0.1583],\n",
       "        [ 0.0538, -0.5639],\n",
       "        [ 0.3546,  0.0099],\n",
       "        [ 0.1720,  0.2146],\n",
       "        [ 0.2468, -0.0498],\n",
       "        [ 0.1716,  0.0061],\n",
       "        [ 0.0315, -0.1518],\n",
       "        [ 0.1567,  0.0910],\n",
       "        [ 0.2617,  0.0378],\n",
       "        [ 0.3030,  0.1635],\n",
       "        [ 0.2562, -0.1694],\n",
       "        [ 0.0322, -0.0254],\n",
       "        [ 0.3319, -0.0425],\n",
       "        [ 0.3415,  0.1019],\n",
       "        [ 0.5443,  0.0410],\n",
       "        [ 0.2463, -0.0489],\n",
       "        [ 0.2101, -0.1463],\n",
       "        [ 0.5497,  0.0603],\n",
       "        [ 0.1768,  0.0570],\n",
       "        [ 0.4695,  0.2415],\n",
       "        [ 0.3106,  0.3136],\n",
       "        [ 0.2150, -0.2025],\n",
       "        [ 0.3638,  0.0986],\n",
       "        [ 0.0825, -0.0146],\n",
       "        [ 0.2939, -0.1234],\n",
       "        [ 0.1673, -0.0813],\n",
       "        [ 0.1968, -0.1756],\n",
       "        [ 0.1644,  0.2042],\n",
       "        [ 0.0044, -0.0088],\n",
       "        [ 0.2984,  0.0595],\n",
       "        [ 0.2600, -0.0238],\n",
       "        [ 0.2343,  0.2755],\n",
       "        [ 0.1559,  0.1624],\n",
       "        [ 0.4113,  0.0732],\n",
       "        [ 0.1674, -0.5165],\n",
       "        [ 0.3914,  0.2100],\n",
       "        [ 0.2293, -0.0848],\n",
       "        [ 0.3598,  0.1883],\n",
       "        [ 0.1690,  0.2078],\n",
       "        [ 0.0068,  0.1241],\n",
       "        [ 0.1537, -0.1367],\n",
       "        [ 0.3147,  0.0106],\n",
       "        [ 0.3045, -0.0599],\n",
       "        [ 0.2835,  0.2024],\n",
       "        [ 0.2085,  0.0977],\n",
       "        [ 0.2026, -0.2012],\n",
       "        [ 0.2050, -0.0906],\n",
       "        [ 0.2447,  0.0871],\n",
       "        [ 0.3379,  0.0861],\n",
       "        [ 0.3535,  0.1015],\n",
       "        [ 0.2729,  0.0682],\n",
       "        [ 0.3807,  0.0644],\n",
       "        [ 0.2349,  0.1606],\n",
       "        [ 0.3321, -0.5284],\n",
       "        [ 0.2771,  0.0895],\n",
       "        [ 0.1739, -0.3253],\n",
       "        [ 0.2762, -0.0067],\n",
       "        [ 0.1843,  0.1506],\n",
       "        [ 0.3389,  0.2318],\n",
       "        [ 0.2472,  0.1205],\n",
       "        [ 0.1737, -0.2026],\n",
       "        [ 0.2077, -0.1144],\n",
       "        [ 0.0567, -0.0203],\n",
       "        [ 0.3092,  0.1895]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[ 0.0879, -0.0800],\n",
       "         [ 0.1675, -0.1584],\n",
       "         [ 0.4839, -0.0300],\n",
       "         [ 0.1695,  0.2763],\n",
       "         [ 0.0508, -0.2329],\n",
       "         [ 0.1382, -0.1222],\n",
       "         [-0.0318, -0.0507],\n",
       "         [ 0.3662,  0.1041],\n",
       "         [ 0.2445, -0.2180],\n",
       "         [ 0.1242, -0.0656],\n",
       "         [ 0.4090,  0.0185],\n",
       "         [-0.0245, -0.0287],\n",
       "         [ 0.2556, -0.2059],\n",
       "         [ 0.2417, -0.1219],\n",
       "         [ 0.1659,  0.0105],\n",
       "         [ 0.3586,  0.0056],\n",
       "         [ 0.0675,  0.0558],\n",
       "         [ 0.4029, -0.0824],\n",
       "         [ 0.4204,  0.1177],\n",
       "         [ 0.2633,  0.2190],\n",
       "         [ 0.2416,  0.6000],\n",
       "         [ 0.2269, -0.1454],\n",
       "         [ 0.3465, -0.2412],\n",
       "         [ 0.0892,  0.0506],\n",
       "         [ 0.5035,  0.0732],\n",
       "         [-0.0602, -0.4913],\n",
       "         [ 0.2417,  0.0592],\n",
       "         [ 0.3030,  0.2401],\n",
       "         [-0.0476,  0.0693],\n",
       "         [ 0.3784,  0.2068],\n",
       "         [ 0.3485, -0.1336],\n",
       "         [ 0.3404,  0.1547],\n",
       "         [ 0.0857, -0.0767],\n",
       "         [ 0.3591, -0.0752],\n",
       "         [ 0.3023,  0.0453],\n",
       "         [ 0.1805, -0.0210],\n",
       "         [ 0.3648,  0.0338],\n",
       "         [ 0.2042,  0.2634],\n",
       "         [ 0.2298,  0.2446],\n",
       "         [ 0.5023,  0.1359],\n",
       "         [ 0.2112, -0.0210],\n",
       "         [ 0.1460,  0.0664],\n",
       "         [ 0.4396, -0.0317],\n",
       "         [ 0.2945,  0.0272],\n",
       "         [ 0.1191, -0.0547],\n",
       "         [ 0.1413, -0.1335],\n",
       "         [ 0.3244, -0.0366],\n",
       "         [ 0.0876,  0.2162],\n",
       "         [ 0.3066, -0.1840],\n",
       "         [ 0.2068, -0.3749],\n",
       "         [ 0.1266, -0.0865],\n",
       "         [ 0.0829,  0.0966],\n",
       "         [ 0.3855,  0.0207],\n",
       "         [ 0.1429, -0.5530],\n",
       "         [ 0.3063,  0.0194],\n",
       "         [ 0.1143, -0.0569],\n",
       "         [ 0.2312,  0.0198],\n",
       "         [ 0.3687, -0.1499],\n",
       "         [ 0.1581,  0.0553],\n",
       "         [ 0.2749, -0.0342],\n",
       "         [-0.0131,  0.1376],\n",
       "         [ 0.2765, -0.0856],\n",
       "         [ 0.2622, -0.0869],\n",
       "         [ 0.2825,  0.1426]], grad_fn=<AddmmBackward>),\n",
       " 'loss': tensor(0.7037, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6968, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    num_epochs=config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6930 ||: 100%|██████████| 32/32 [01:16<00:00,  1.94s/it]\n",
      "loss: 0.6321 ||: 100%|██████████| 32/32 [01:14<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import DataIterator\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit # the sigmoid function\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logits\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator,\n",
    "                                   total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "# iterate over the dataset without changing its order\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:48<00:00,  3.47s/it]\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "train_preds = predictor.predict(train_ds) \n",
    "#test_preds = predictor.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Final Note on Predictors\n",
    "\n",
    "AllenNLP also provides predictors that take strings as input and outputs model predictions. They're handy if you want to create simple demo or need to make predictions on entirely new data, but since we've already read data as datasets and want to preserve their order, we didn't use them above.\n",
    "\n",
    "Need to make a reader that will convert the string in the format the that model understands. In this case, similar to the train data. The model is looking for an instance not a string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'reddit_thread_label_small_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:01, 1105.75it/s]\n",
      "100%|██████████| 32/32 [02:50<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.88\n"
     ]
    }
   ],
   "source": [
    "test_ds = reader.read(fname) \n",
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "test_preds = predictor.predict(test_ds) \n",
    "df=pd.read_pickle(fname)\n",
    "score = np.argmax(test_preds,axis=1)\n",
    "df['temp'] = df[label_cols].apply(lambda x: np.array(x[label_cols]),axis=1)\n",
    "df['label'] = df.temp.apply(lambda x:np.argmax(x))\n",
    "print(f'Score {sum(df.label == score)/len(score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in label_cols:\n",
    "#     score = np.argmax(train_preds,axis=1)\n",
    "#     argument = np.argmax(np.array(label_cols) == label)\n",
    "#     score = np.sum(score == argument)/len(train_preds)\n",
    "    \n",
    "#     print(f'{label} has a score of {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "\n",
    "# class NewDatasetReader(DatasetReader):\n",
    "#     def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "#                  token_indexers: Dict[str, TokenIndexer] = None,\n",
    "#                  max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "#         super().__init__(lazy=False)\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "#         self.max_seq_len = max_seq_len\n",
    "\n",
    "#     @overrides\n",
    "#     def text_to_instance(self, tokens: List[Token],\n",
    "#                          labels: np.ndarray=None,\n",
    "#                         id: str=None,) -> Instance:\n",
    "#         sentence_field = TextField(tokens, self.token_indexers)\n",
    "#         fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "#         fields[\"label\"] = ArrayField(array=labels)\n",
    "    \n",
    "#         id_field = MetadataField(np.random.randint(1000))\n",
    "#         fields[\"id\"] = id_field\n",
    "#         return Instance(fields)\n",
    "    \n",
    "#     @overrides\n",
    "#     def _read(self, string: str) -> Iterator[Instance]:\n",
    "#         # Imports data into the tokenizer\n",
    "#         #for string in strings:\n",
    "#         print(string)\n",
    "#         yield self.text_to_instance([Token(x) for x in self.tokenizer(str(string))])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_reader = NewDatasetReader(\n",
    "#     tokenizer=tokenizer,\n",
    "#     token_indexers={\"tokens\": token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_reader.read(\"this tutorial was great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.predictors import SentenceTaggerPredictor\n",
    "# predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "# tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "# tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "# print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
